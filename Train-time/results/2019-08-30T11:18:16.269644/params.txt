batch_size = 128
alpha = 0.1
epsilon = 0.0001
num_units = 300
n_hidden_layers = 1
num_epochs = 500
dropout_in = 0.2
dropout_hidden = 0.5
activation = binary_net.binary_tanh_unit
binary = True
stochastic = False
H = 1.0
W_LR_scale = Glorot
LR_start = 0.005
LR_fin = 5e-07
LR_decay = 0.9817479430199844
save_path = None
shuffle_parts = 1
noise = u
nalpha = 50
train_set_size = 510000
validation_set_size = 10000
test_set_size = 10000
