batch_size = 128
alpha = 0.1
epsilon = 0.0001
num_units = 300
n_hidden_layers = 1
num_epochs = 500
dropout_in = 0.2
dropout_hidden = 0.5
activation = binary_net.binary_tanh_unit
binary = True
stochastic = False
H = 1.0
W_LR_scale = Glorot
LR_start = 0.01
LR_fin = 1e-05
LR_decay = 0.9862794856312105
save_path = None
shuffle_parts = 1
noise = u
nalpha = 20
train_set_size = 210000
validation_set_size = 10000
test_set_size = 10000
